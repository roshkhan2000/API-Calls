# This script loads Azure credentials and configuration from environment variables.
# It calculates the previous month based on the current date.
# It then lists all zipped blobs in Azure storage for that month. Each blob is downloaded into memory and opened as a zip file.
# The script extracts any Excel files found inside the zip archives. All Excel files are combined into a single Pandas DataFrame.
# New columns such as total price and month are created from existing data.
# The data is then aggregated by month and region + is saved as a CSV file and uploaded back to Azure storage.


# os to interact with operating system
# pandas for data transformation
# datetime for datetime functions
# io to write in-memory files
# zipfile to interact with zip files 
# BlobServiceClient to interact with Azure blob
# dotenv to import sensitive keys and info
import os
import pandas as pd
from datetime import datetime, timedelta
from io import BytesIO
from zipfile import ZipFile

from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

# read dot env files and store secret keys, container name, and container file paths as variables
load_dotenv()

AZURE_CONN = os.getenv("AZURE_CONNECTION_STRING")
CONTAINER = os.getenv("AZURE_CONTAINER")

INPUT_PREFIX = os.getenv("INPUT_PREFIX", "monthly_data/raw/")
OUTPUT_PREFIX = os.getenv("OUTPUT_PREFIX", "monthly_data/summary/")

# define today as today's date
# first_day day as first day of current month
# last_month as first_day minus 1 day
# month_str as year and month format (date truncated) 
today = datetime.utcnow().date()
first_day = today.replace(day=1)
last_month = first_day - timedelta(days=1)
month_str = last_month.strftime("%Y-%m")

# find blob from dotenv file and define container name / filepath
blob_service = BlobServiceClient.from_connection_string(AZURE_CONN)
container_client = blob_service.get_container_client(CONTAINER)

# create and empty list
all_dfs = []

# list all the blobs available in the container with defined input prefix and dates
blobs = container_client.list_blobs(name_starts_with=f"{INPUT_PREFIX}{month_str}/")

# for loop: for each blob in the list above, make a call to the blob
# downlaod blob data in-memory
# if files with with .xlsx in blob then open and read them, and append each to the empty list above
# close process when done and print a message if nothing is found
for blob in blobs:
    blob_client = container_client.get_blob_client(blob)
    stream = BytesIO(blob_client.download_blob().readall())
    
    with ZipFile(stream) as z:
        for file_name in z.namelist():
            if file_name.endswith(".xlsx"):
                with z.open(file_name) as f:
                    df = pd.read_excel(f)
                    all_dfs.append(df)

if not all_dfs:
    print("No data for last month")
    exit()

# concatenate all_dfs together 
# create a df from order_date, month, and total_price from data
# aggregate to month and region level 
data = pd.concat(all_dfs, ignore_index=True)

data["order_date"] = pd.to_datetime(data["order_date"])
data["month"] = data["order_date"].dt.to_period("M")
data["total_price"] = data["quantity"] * data["unit_price"]

summary = (
    data.groupby(["month", "region"])
    .agg(
        total_orders=("order_id", "count"),
        total_revenue=("total_price", "sum"),
        avg_order_value=("total_price", "mean")
    )
    .reset_index()
)

# define output prefix / file path and csv name
# upload back to container but under a different prefix / file name 
output_file = f"{OUTPUT_PREFIX}monthly_summary_{month_str}.csv"
container_client.upload_blob(
    name=output_file,
    data=summary.to_csv(index=False).encode("utf-8"),
    overwrite=True
)

print(f"Uploaded monthly summary to {output_file}")
