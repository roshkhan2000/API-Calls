# os for interacting with operating system e.g. create files
# json for transforming json responses 
# datetime for date / time functions
# io for ???
import os
import json
from datetime import datetime, timedelta
from io import StringIO

# boto3 is SDK for AWS 
# pandas for data transformation
# dotenv to load secret keys 
import boto3
import pandas as pd
from dotenv import load_dotenv

# read dotenv file
# define different keys as variables 
# input and ouput prefix for defining where in the bucket to store the data
load_dotenv()

AWS_KEY = os.getenv("AWS_ACCESS_KEY")
AWS_SECRET = os.getenv("AWS_SECRET_KEY")
BUCKET = os.getenv("AWS_BUCKET_NAME")

INPUT_PREFIX = os.getenv("INPUT_PREFIX", "sales/raw/")
OUTPUT_PREFIX = os.getenv("OUTPUT_PREFIX", "sales/processed/")

# idk what end date or start date are doing
end_date = datetime.utcnow().date() - timedelta(days=1)
start_date = end_date - timedelta(days=6)

# definign your information to be able to connect to the bucket
s3 = boto3.client(
    "s3",
    aws_access_key_id=AWS_KEY,
    aws_secret_access_key=AWS_SECRET
)

# making a dataframe 
all_records = []

current_date = start_date

# iterative loop until the below requirement is met 
while current_date <= end_date:

# deine your date and file name
    date_path = current_date.strftime("year=%Y/month=%m/day=%d")
    file_key = f"{INPUT_PREFIX}{date_path}/sales.json"

# actually connecting to the bucket and calling data
# read from the body convert it to json 
# except define the condition needed to get out of the loop

    try:
        response = s3.get_object(
            Bucket=BUCKET,
            Key=file_key
        )

        data = response["Body"].read().decode("utf-8")
        daily_records = json.loads(data)

        all_records.extend(daily_records)

        print(f"Loaded data for {current_date}")

    except s3.exceptions.NoSuchKey:
        print(f"No data for {current_date}")

    current_date += timedelta(days=1)

# comvert json response to a data drame
# if nothing was iploaded then say no data found
df = pd.DataFrame(all_records)

if df.empty:
    print("No data found for last 7 days")
    exit()

# order date column as the order timespamt and converting it to date
# calculating revenye from quanitity and price
# aggrgatgin by order date and country where total orders is the ocunt of order id , total revenue is the sum of revenu etc
df["order_date"] = pd.to_datetime(df["order_timestamp"]).dt.date

df["revenue"] = df["quantity"] * df["price"]

summary = (
    df.groupby(["order_date", "country"])
    .agg(
        total_orders=("order_id", "count"),
        total_revenue=("revenue", "sum"),
        avg_order_value=("revenue", "mean")
    )
    .reset_index()
)

## buffer typicalyy is where to store the csv and you are storing csv somewhere
buffer = StringIO()
summary.to_csv(buffer, index=False)

# upload to another bucket 
output_key = f"{OUTPUT_PREFIX}weekly_sales_summary.csv"

s3.put_object(
    Bucket=BUCKET,
    Key=output_key,
    Body=buffer.getvalue().encode(),
    ContentType="text/csv"
)

print(f"Summary written to s3://{BUCKET}/{output_key}")


This script loads AWS credentials from environment variables and processes sales data from the last 7 complete days. It loops over daily S3 partitions, downloads JSON files, and combines them into a single dataset. The data is converted into a Pandas DataFrame, enriched with calculated fields like revenue and order date, and aggregated by date and country. Finally, it writes the summary to an in-memory CSV buffer and uploads it back to S3.

