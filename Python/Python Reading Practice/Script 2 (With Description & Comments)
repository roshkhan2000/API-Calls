# This script loads AWS credentials from environment variables and processes sales data from the last 7 complete days. 
# It loops over daily S3 partitions, downloads JSON files, and combines them into a single dataset. 
# The data is converted into a Pandas DataFrame, enriched with calculated fields like revenue and order date, and aggregated by date and country. 
# Finally, it writes the summary to an in-memory CSV buffer and uploads it back to S3.


# load os for interacting with operating system
# load json to convert json strings to python objects
# load datetime to perform datetime functions
# load StringIO to create in-memory file objects for saving CSVs
# load boto3 as a SKD to connect to AWS client
# load pandas for data tansformation
# dotenv to read secret keys and sensitive info from local files
import os
import json
from datetime import datetime, timedelta
from io import StringIO

import boto3
import pandas as pd
from dotenv import load_dotenv

# read your dotenv file that has secret keys and sensitive info
# define above info in to respective variables
# define start date and end date and last 7 days not including today
load_dotenv()

AWS_KEY = os.getenv("AWS_ACCESS_KEY")
AWS_SECRET = os.getenv("AWS_SECRET_KEY")
BUCKET = os.getenv("AWS_BUCKET_NAME")

INPUT_PREFIX = os.getenv("INPUT_PREFIX", "sales/raw/")
OUTPUT_PREFIX = os.getenv("OUTPUT_PREFIX", "sales/processed/")

end_date = datetime.utcnow().date() - timedelta(days=1)
start_date = end_date - timedelta(days=6)

# create AWS s3 client to connect to AWS buckets
# all_records creates an empty list
# create a loop: while current date is less than or equal to end date, create date partitions in file names 
# try: calls the AWS s3 client with the defined info
# The response body is read as bytes and decoded into a string. and then added to all_records
# except defines what to do when no data is found
# loop is closed when current_date += timedelta(days=1)
s3 = boto3.client(
    "s3",
    aws_access_key_id=AWS_KEY,
    aws_secret_access_key=AWS_SECRET
)

all_records = []

current_date = start_date

while current_date <= end_date:

    date_path = current_date.strftime("year=%Y/month=%m/day=%d")
    file_key = f"{INPUT_PREFIX}{date_path}/sales.json"

    try:
        response = s3.get_object(
            Bucket=BUCKET,
            Key=file_key
        )

        data = response["Body"].read().decode("utf-8")
        daily_records = json.loads(data)

        all_records.extend(daily_records)

        print(f"Loaded data for {current_date}")

    except s3.exceptions.NoSuchKey:
        print(f"No data for {current_date}")

    current_date += timedelta(days=1)

# all_records python object is then converted to a dataframe
# if nothign is found in these files and therefore the dataframe, print below message and exit
# order_date column is defined as the order_timestamp column 
# revenue is calculated by quantity * price
# summary created aggregates the data
df = pd.DataFrame(all_records)

if df.empty:
    print("No data found for last 7 days")
    exit()

df["order_date"] = pd.to_datetime(df["order_timestamp"]).dt.date

df["revenue"] = df["quantity"] * df["price"]

summary = (
    df.groupby(["order_date", "country"])
    .agg(
        total_orders=("order_id", "count"),
        total_revenue=("revenue", "sum"),
        avg_order_value=("revenue", "mean")
    )
    .reset_index()
)

# buffer creates in memory saves
# files are saved in memory then uploaded in to another prefix / file in the bucket
# print once everything is accomplished 
buffer = StringIO()
summary.to_csv(buffer, index=False)

output_key = f"{OUTPUT_PREFIX}weekly_sales_summary.csv"

s3.put_object(
    Bucket=BUCKET,
    Key=output_key,
    Body=buffer.getvalue().encode(),
    ContentType="text/csv"
)

print(f"Summary written to s3://{BUCKET}/{output_key}")
