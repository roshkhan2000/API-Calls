import os
import gzip
import json
from datetime import datetime, timedelta
from io import BytesIO, StringIO

import boto3
import pandas as pd
from dotenv import load_dotenv

load_dotenv()

AWS_KEY = os.getenv("AWS_ACCESS_KEY")
AWS_SECRET = os.getenv("AWS_SECRET_KEY")
BUCKET = os.getenv("AWS_BUCKET")

LOG_PREFIX = os.getenv("LOG_PREFIX", "logs/raw/")
REPORT_PREFIX = os.getenv("REPORT_PREFIX", "logs/reports/")

run_date = datetime.utcnow().date() - timedelta(days=1)

start_hour = 0
end_hour = 23

s3 = boto3.client(
    "s3",
    aws_access_key_id=AWS_KEY,
    aws_secret_access_key=AWS_SECRET
)

records = []

for hour in range(start_hour, end_hour + 1):

    hour_path = f"date={run_date}/hour={hour:02d}"
    file_key = f"{LOG_PREFIX}{hour_path}/events.json.gz"

    try:
        response = s3.get_object(
            Bucket=BUCKET,
            Key=file_key
        )

        compressed = response["Body"].read()

        with gzip.GzipFile(fileobj=BytesIO(compressed)) as f:
            content = f.read().decode("utf-8")

        lines = content.splitlines()

        for line in lines:
            if line.strip():
                records.append(json.loads(line))

        print(f"Processed hour {hour}")

    except s3.exceptions.NoSuchKey:
        print(f"Missing file for hour {hour}")

df = pd.DataFrame(records)

if df.empty:
    print("No log data found")
    exit()

df["event_time"] = pd.to_datetime(df["timestamp"])
df["event_date"] = df["event_time"].dt.date

summary = (
    df.groupby(["event_date", "event_type"])
    .agg(
        total_events=("event_id", "count"),
        unique_users=("user_id", "nunique"),
        error_events=("status", lambda x: (x == "error").sum())
    )
    .reset_index()
)

buffer = StringIO()
summary.to_csv(buffer, index=False)

output_key = f"{REPORT_PREFIX}daily_event_report_{run_date}.csv"

s3.put_object(
    Bucket=BUCKET,
    Key=output_key,
    Body=buffer.getvalue().encode(),
    ContentType="text/csv"
)

print(f"Report written to s3://{BUCKET}/{output_key}")
